# Datawhale AI 夏令营 Task2 日记

## 一、深度学习是什么

1. 深度学习（Deep Learning）是机器学习的一个分支，它使用神经网络模拟人脑的学习方式，从大量数据中自动学习和提取特征，进行预测和决策。

2. 神经元模型：模拟生物神经元行为的计算模型，包括输入、权重、激活函数和输出。

    * 激活函数：ReLU, Leaky ReLU, Tanh, Binary Step, Linear, SELU, ELU, Sigmoid, Parametric ReLU

3. 神经网络：由基本分神经元组合形成的多层次的网络结构

4. 优化技术和算法：

    1. 激活函数的改进
    2. 权重初始化方法
    3. 正则化技术
    4. 梯度下降的变种
    5. 开发新的网络结构（目前主要进行的研究）

5. 传统机器学习：需要人工设计特征，费时费力

    深度学习：自动从原始数据中学习到有用的特征，可以实现端到端的学习

6. 深度学习不能解决所有的问题：深度学习依赖与大量的数据，而对于某些领域或问题，获取足够的高质量数据是很难的。

## 二、深度学习如何训练的

1. 梯度下降算法：沿着梯度的反方向调整模型参数，使模型参数朝着减少损失的方向移动
    1. 在训练循环中，我们首先加载一小批量数据，将其输入到神经网络中进行前向传播，计算出网络的输出。
    2. 然后，我们使用损失函数来计算当前批次的损失，并通过反向传播算法计算损失函数关于每个参数的梯度。
    3. 根据梯度调整模型参数以减少损失。
    4. 当数据集非常大时，一次性处理所有数据可能会导致内存不足或计算过于缓慢。通过将数据分成小批量，我们可以更频繁地更新模型参数，这使得训练过程更加高效。
2. PyTorch 训练代码
    1. 准备数据 `dataloader` 和模型 `model` 
    2. 将模型调整为训练模式 `model.train()` 
    3. 加载数据并输入到模型进行前向传播 `pred = model(input)`
    4. 计算损失 `loss = criterion(pred, target)` 
    5. 更新参数 `loss.backward()` `optimizer.step()` 

## 三、深度学习与迁移学习

1. 迁移学习：将已在一个任务上学到的知识（如模型参数、特征表示等）应用到另一个相关任务上。

    * 优点：可以在数据稀缺的情况下获得较好的效果。
    * 通常使用在大规模数据集上预训练的模型作为起点，例如在ImageNet数据集上预训练的卷积神经网络（CNN）。在预训练模型的基础上，使用少量标记数据对模型进行微调，以适应新任务。

2. ImageNet：包含超过1400万张注释过的图像，分布在超过2.2万个类别中。是深度学习模型训练和评估的理想数据集。

    * 提供了一个标准的性能基础，可以通过在ImageNet上的表现来比较不同模型的性能。
    * 许多在imageNet上预训练的模型被用做迁移学习的起点，这些模型在新任务上表现出色。

3. 迁移学习的实现方法

    1. 在一个大规模的数据集上预训练一个深度学习模型，捕捉通用的特征表示。

        * `timm` 库中提供了很多网络的预训练模型，下面的代码是加载了一个预训练的ResNet-18模型。

        ```python
        model = timm.create_model('resnet18', pretrained=True, num_classes=2)
        ```

    2. 将这个预训练模型作为起点，在目标任务上进行进一步的训练以提升模型的性能。

        * 替换模型的输出层，以匹配目标任务的类别和类型
        * 冻结预训练模型中的大部分层，防止在微调过程中破坏这些层学习到的通用特征。（非必需）
        * 使用目标任务的数据集进行训练。训练时可能会使用比预训练时更低的学习率，以免过度拟合目标数据集。

## 四、常见的图像分类网络

1. AlexNet：由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton设计，在2012年的ImageNet大规模视觉识别挑战赛（ILSVRC）中取得了突破性的成绩。
    * 包含八个层次结构，前五个是卷积层，其中一些后跟最大池化层，最后三个是全连接层。
2. ResNet：由微软研究院的研究员何恺明等人提出，在2015年的ImageNet图像识别大赛中取得了冠军，并在深度学习领域产生了重大影响。
    * 引入了残差学习的概念，允许训练非常深的网络，从而缓解了深度神经网络训练中的梯度消失和梯度爆炸问题。
3. EfficientNet：通过一种新颖的网络缩放方法来提升模型的性能和效率。
    * 核心是其 compound scaling 方法，该方法通过一个复合系数统一缩放网络的深度、宽度和分辨率，而不同于过去任意选择深度、宽度或分辨率的增加。
    * EfficientNet的复合缩放方法的直觉在于，如果输入图像更大，网络就需要更多的层来增加感受野，以及更多的通道来捕捉更细粒度的模式。